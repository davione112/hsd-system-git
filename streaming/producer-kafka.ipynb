{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c3cfe5-dd45-47c6-8590-f7e162371112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from apiclient.discovery import build\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from confluent_kafka import Producer\n",
    "from kafka import KafkaProducer\n",
    "import logging\n",
    "import socket\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89b65f5-5995-4797-8a88-ab866e1ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build service for calling the Youtube API:\n",
    "## Arguments that need to passed to the build function\n",
    "DEVELOPER_KEY = \"AIzaSyDbt-xdAOjDhJghQGVMxfbsSiSyCFJr1Jw\" \n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "# video_link = \"https://www.youtube.com/watch?v=-1X6Ak94Acs\"\n",
    "video_link = \"https://youtu.be/Wb_-uXCyeYI\"\n",
    "   \n",
    "## creating Youtube Resource Object\n",
    "youtube_service = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                                        developerKey = DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72dc4a26-bf18-4ec1-8628-5876d6d8d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a producer\n",
    "def create_producer():\n",
    "    try:\n",
    "        producer = Producer({\"bootstrap.servers\": \"localhost:9092\",\n",
    "                             \"client.id\": socket.gethostname(),\n",
    "                             \"enable.idempotence\": True,  # EOS processing\n",
    "                             \"compression.type\": \"lz4\",\n",
    "                             \"batch.size\": 64000,\n",
    "                             \"linger.ms\": 10,\n",
    "                             \"acks\": \"all\",  # Wait for the leader and all ISR to send response back\n",
    "                             \"retries\": 5,\n",
    "                             \"delivery.timeout.ms\": 1000})  # Total time to make retries\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Couldn't create the producer\")\n",
    "        producer = None\n",
    "    return producer\n",
    "\n",
    "### Function to get youtube video id.\n",
    "# source:\n",
    "# https://stackoverflow.com/questions/45579306/get-youtube-video-url-or-youtube-video-id-from-a-string-using-regex\n",
    "def get_id(url):\n",
    "    u_pars = urlparse(url)\n",
    "    quer_v = parse_qs(u_pars.query).get('v')\n",
    "    if quer_v:\n",
    "        return quer_v[0]\n",
    "    pth = u_pars.path.split('/')\n",
    "    if pth:\n",
    "        return pth[-1]\n",
    "    \n",
    "def get_comments(url,num_comment):\n",
    "  response = youtube_service.commentThreads().list(\n",
    "      part='snippet',\n",
    "      maxResults=num_comment,\n",
    "      textFormat='plainText',\n",
    "      order='time',\n",
    "      videoId=get_id(url)\n",
    "  ).execute()\n",
    "\n",
    "  results = response.get('items',[])\n",
    "\n",
    "  # extract video comments\n",
    "  authors=[]\n",
    "  authorUrls=[]\n",
    "  texts=[]\n",
    "  datetimes=[]\n",
    "\n",
    "  for item in results:\n",
    "    authors.append(item['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "    authorUrls.append(item['snippet']['topLevelComment']['snippet']['authorChannelUrl'])\n",
    "    texts.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    datetimes.append(item['snippet']['topLevelComment']['snippet']['updatedAt'])\n",
    "\n",
    "  dataFrame = pd.DataFrame({'datetime':datetimes,'author':authors,'authorUrl':authorUrls,'comment':texts})\n",
    "\n",
    "  return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a68fe-bdb2-4148-9275-0a041275e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce message\n",
      "b'{\"author\": \"Tuy\\\\u1ec1n V\\\\u0103n H\\\\u00f3a\", \"datetime\": \"2021-10-07T19:35:06Z\", \"raw_comment\": \"Highlights Trung Qu\\\\u1ed1c vs Vi\\\\u1ec7t Nam: https://youtu.be/HRGLauElf6g\", \"clean_comment\": \"highlights trung_qu\\\\u1ed1c v\\\\u1edbi vi\\\\u1ec7t_nam\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Phu Vu\", \"datetime\": \"2021-11-09T02:23:34Z\", \"raw_comment\": \"th\\\\u01b0\\\\u01a1ng l\\\\u1eafm vi\\\\u1ec7t nam \\\\u01a1i\", \"clean_comment\": \"th\\\\u01b0\\\\u01a1ng l\\\\u1eafm vi\\\\u1ec7t nam \\\\u01a1i\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"anh khoa\", \"datetime\": \"2021-11-02T13:22:01Z\", \"raw_comment\": \"\\\\u0111\\\\u00e1 h\\\\u1ed3i n\\\\u00e0o d\\\\u1ecb\", \"clean_comment\": \"\\\\u0111\\\\u00e1 h\\\\u1ed3i n\\\\u00e0o v\\\\u1eady\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"m\\\\u00e8o /gacha\", \"datetime\": \"2021-10-30T23:57:25Z\", \"raw_comment\": \"C\\\\u1ed1 l\\\\u00ean viet nam\", \"clean_comment\": \"c\\\\u1ed1 l\\\\u00ean viet nam\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Nguy\\\\u1ec5n D\\\\u01b0\\\\u01a1ng Tu\\\\u1ea5n H\\\\u01b0ng\", \"datetime\": \"2021-10-26T06:28:48Z\", \"raw_comment\": \"L\\\\u01b0\\\\u1ee3t v\\\\u1ec1 Vi\\\\u1ec7t Nam vs Trung Qu\\\\u1ed1c k\\\\u1ebft qu\\\\u1ea3 l\\\\u00e0 3-2 d\\\\u00e0nh cho Vi\\\\u1ec7t Nam\", \"clean_comment\": \"l\\\\u01b0\\\\u1ee3t v\\\\u1ec1 vi\\\\u1ec7t_nam v\\\\u1edbi trung qu\\\\u1ed1c k\\\\u1ebft_qu\\\\u1ea3 l\\\\u00e0 d\\\\u00e0nh cho vi\\\\u1ec7t_nam\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"B\\\\u00f9i Th\\\\u1ee7y\", \"datetime\": \"2021-10-24T04:31:54Z\", \"raw_comment\": \"\\\\\"TU\\\\u1ea4N ANH - r\\\\u1ea5t \\\\u0111i\\\\u1ec1m \\\\u0111\\\\u1ea1m! \\\\\"\\\\nTh\\\\u01b0\\\\u01a1ng gh\\\\u00ea.\", \"clean_comment\": \"tu\\\\u1ea5n anh r\\\\u1ea5t \\\\u0111i\\\\u1ec1m_\\\\u0111\\\\u1ea1m th\\\\u01b0\\\\u01a1ng gh\\\\u00ea\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Leigh Andrews\", \"datetime\": \"2021-10-23T15:47:53Z\", \"raw_comment\": \"\\\\u0110\\\\u00e0i Loan c\\\\u0103ng qu\\\\u00e1\\\\ud83d\\\\ude00\\\\ud83d\\\\ude01\\\\n\\\\u0111\\\\u00e1nh nhau to r\\\\u1ed3i\\\\ud83d\\\\udc74\", \"clean_comment\": \"\\\\u0111\\\\u00e0i_loan c\\\\u0103ng qu\\\\u00e1 \\\\u0111\\\\u00e1nh nhau to r\\\\u1ed3i\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Hai Dang\", \"datetime\": \"2021-10-22T14:11:04Z\", \"raw_comment\": \"D\\\\u00f5 r\\\\u00e0ng l\\\\u1ecbch vi\", \"clean_comment\": \"d\\\\u00f5 r\\\\u00e0ng l\\\\u1ecbch_vi\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Vi\\\\u1ec7t Jonny\", \"datetime\": \"2021-10-22T08:11:35Z\", \"raw_comment\": \"Super Idol\", \"clean_comment\": \"si\\\\u00eau th\\\\u1ea7n_t\\\\u01b0\\\\u1ee3ng\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Beo Tom\", \"datetime\": \"2021-10-21T10:43:52Z\", \"raw_comment\": \"Viet nam c\\\\u00f4 len\", \"clean_comment\": \"viet nam c\\\\u00f4 len\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Ngoc bao Ngo\", \"datetime\": \"2021-10-21T06:00:41Z\", \"raw_comment\": \"tuy t\\\\u00f4i kh\\\\u00f4ng h\\\\u1ee9 th\\\\u00fa b\\\\u00f3ng \\\\u0111\\\\u00e1 nh\\\\u01b0 \\\\nC\\\\u00e1c b\\\\u1ea1n h\\\\u00e3y c\\\\u1ed1 l\\\\u00ean thua c\\\\u0169ng \\\\u0111\\\\u01b0\\\\u1ee3c kh\\\\u00f4ng sao c\\\\u1ea3 \\\\nmi\\\\u1ec5n l\\\\u00e0 kh\\\\u00f4ng b\\\\u1ecb tr\\\\u1ea5n th\\\\u01b0\\\\u01a1ng l\\\\u00e0 \\\\u0111\\\\u01b0\\\\u1ee3c \\\\nkh\\\\u00f4ng c\\\\u1ea7n ph\\\\u1ea3i c\\\\u1ed1 qu\\\\u00e1 l\\\\u00e0m j l\\\\u1ea1i \\\\u1ea3nh h\\\\u01b0\\\\u1edfng \\\\u0111\\\\u1ebfn s\\\\u1ee9c kho\\\\u1ebb th\\\\u00ec kh\\\\u1ed5 l\\\\u1eafm\", \"clean_comment\": \"tuy t\\\\u00f4i kh\\\\u00f4ng h\\\\u1ee9 th\\\\u00fa b\\\\u00f3ng_\\\\u0111\\\\u00e1 nh\\\\u01b0 c\\\\u00e1c b\\\\u1ea1n h\\\\u00e3y c\\\\u1ed1 l\\\\u00ean thua c\\\\u0169ng \\\\u0111\\\\u01b0\\\\u1ee3c kh\\\\u00f4ng sao c\\\\u1ea3 mi\\\\u1ec5n_l\\\\u00e0 kh\\\\u00f4ng b\\\\u1ecb tr\\\\u1ea5n th\\\\u01b0\\\\u01a1ng l\\\\u00e0 \\\\u0111\\\\u01b0\\\\u1ee3c kh\\\\u00f4ng c\\\\u1ea7n ph\\\\u1ea3i c\\\\u1ed1 qu\\\\u00e1 l\\\\u00e0m g\\\\u00ec l\\\\u1ea1i \\\\u1ea3nh_h\\\\u01b0\\\\u1edfng \\\\u0111\\\\u1ebfn s\\\\u1ee9c_kho\\\\u1ebb th\\\\u00ec kh\\\\u1ed5 l\\\\u1eafm\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Thanh T\\\\u00f9ng L\\\\u00ea\", \"datetime\": \"2021-10-21T02:48:20Z\", \"raw_comment\": \"trung quoc tuoi gi\", \"clean_comment\": \"trung quoc tuoi gi\", \"label\": 0}'\n"
     ]
    }
   ],
   "source": [
    "# producer = create_producer()\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "response = youtube_service.commentThreads().list(\n",
    "      part='snippet',\n",
    "      maxResults=100,\n",
    "      textFormat='plainText',\n",
    "      order='time',\n",
    "      videoId=get_id(video_link)\n",
    "  ).execute()\n",
    "\n",
    "# response = youtube_service.liveChatMessages().list(\n",
    "#       part='snippet',\n",
    "#       maxResults=100,\n",
    "#       liveChatId=get_id(video_link)\n",
    "#   ).execute()\n",
    "\n",
    "results = response.get('items',[])\n",
    "\n",
    "# extract video comments\n",
    "try:\n",
    "    for item in results:\n",
    "        author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "        authorurl = item['snippet']['topLevelComment']['snippet']['authorChannelUrl']\n",
    "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        datetime = item['snippet']['topLevelComment']['snippet']['updatedAt']\n",
    "        \n",
    "        ## Hate speech detection\n",
    "        # load DNN model\n",
    "        model_path = os.path.abspath('../model/Text_CNN_model_PhoW2V.h5')\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        tknz_path = os.path.abspath('../model/tokenizer.pickle')\n",
    "        with open(tknz_path,\"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "            \n",
    "        # dnn\n",
    "        processed_comment = preprocessing(comment)\n",
    "        seq_comment = tokenizer.texts_to_sequences([processed_comment])\n",
    "        ds_comment = sequence.pad_sequences(seq_comment,maxlen=80)\n",
    "        pred = model.predict(ds_comment)\n",
    "        hsd_dt = pred.argmax(-1)\n",
    "        \n",
    "        record = {\"author\":author,\"datetime\":datetime,\"raw_comment\":comment,\n",
    "                  \"clean_comment\":processed_comment,\"label\":int(hsd_dt[0])}\n",
    "        record = json.dumps(record).encode(\"utf-8\")\n",
    "        print('produce message')\n",
    "        print(record)\n",
    "\n",
    "#         producer.produce(topic=\"hsd\",value=record)\n",
    "        producer.send(topic='hsd',value=record)\n",
    "        time.sleep(5)\n",
    "except KeyboardInterrupt:\n",
    "        print('Stop flush!')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1dcf4-6107-490c-96e5-127b7c00fa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
