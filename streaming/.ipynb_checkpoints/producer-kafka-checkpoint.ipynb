{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c3cfe5-dd45-47c6-8590-f7e162371112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from apiclient.discovery import build\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from confluent_kafka import Producer\n",
    "import logging\n",
    "import socket\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89b65f5-5995-4797-8a88-ab866e1ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build service for calling the Youtube API:\n",
    "## Arguments that need to passed to the build function\n",
    "DEVELOPER_KEY = \"AIzaSyDbt-xdAOjDhJghQGVMxfbsSiSyCFJr1Jw\" \n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "# video_link = \"https://www.youtube.com/watch?v=-1X6Ak94Acs\"\n",
    "video_link = \"https://youtu.be/Wb_-uXCyeYI\"\n",
    "   \n",
    "## creating Youtube Resource Object\n",
    "youtube_service = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
    "                                        developerKey = DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72dc4a26-bf18-4ec1-8628-5876d6d8d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a producer\n",
    "def create_producer():\n",
    "    try:\n",
    "        producer = Producer({\"bootstrap.servers\": \"localhost:9092\",\n",
    "                             \"client.id\": socket.gethostname(),\n",
    "                             \"enable.idempotence\": True,  # EOS processing\n",
    "                             \"compression.type\": \"lz4\",\n",
    "                             \"batch.size\": 64000,\n",
    "                             \"linger.ms\": 10,\n",
    "                             \"acks\": \"all\",  # Wait for the leader and all ISR to send response back\n",
    "                             \"retries\": 5,\n",
    "                             \"delivery.timeout.ms\": 1000})  # Total time to make retries\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Couldn't create the producer\")\n",
    "        producer = None\n",
    "    return producer\n",
    "\n",
    "### Function to get youtube video id.\n",
    "# source:\n",
    "# https://stackoverflow.com/questions/45579306/get-youtube-video-url-or-youtube-video-id-from-a-string-using-regex\n",
    "def get_id(url):\n",
    "    u_pars = urlparse(url)\n",
    "    quer_v = parse_qs(u_pars.query).get('v')\n",
    "    if quer_v:\n",
    "        return quer_v[0]\n",
    "    pth = u_pars.path.split('/')\n",
    "    if pth:\n",
    "        return pth[-1]\n",
    "    \n",
    "def get_comments(url,num_comment):\n",
    "  response = youtube_service.commentThreads().list(\n",
    "      part='snippet',\n",
    "      maxResults=num_comment,\n",
    "      textFormat='plainText',\n",
    "      order='time',\n",
    "      videoId=get_id(url)\n",
    "  ).execute()\n",
    "\n",
    "  results = response.get('items',[])\n",
    "\n",
    "  # extract video comments\n",
    "  authors=[]\n",
    "  authorUrls=[]\n",
    "  texts=[]\n",
    "  datetimes=[]\n",
    "\n",
    "  for item in results:\n",
    "    authors.append(item['snippet']['topLevelComment']['snippet']['authorDisplayName'])\n",
    "    authorUrls.append(item['snippet']['topLevelComment']['snippet']['authorChannelUrl'])\n",
    "    texts.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "    datetimes.append(item['snippet']['topLevelComment']['snippet']['updatedAt'])\n",
    "\n",
    "  dataFrame = pd.DataFrame({'datetime':datetimes,'author':authors,'authorUrl':authorUrls,'comment':texts})\n",
    "\n",
    "  return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7a68fe-bdb2-4148-9275-0a041275e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huy\\anaconda3\\envs\\Homura2\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator Ridge from version 0.22.2.post1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce message\n",
      "b'{\"author\": \"Tuy\\\\u1ec1n V\\\\u0103n H\\\\u00f3a\", \"datetime\": \"2021-10-07T19:35:06Z\", \"raw_comment\": \"Highlights Trung Qu\\\\u1ed1c vs Vi\\\\u1ec7t Nam: https://youtu.be/HRGLauElf6g\", \"clean_comment\": \"highlights trung_qu\\\\u1ed1c v\\\\u1edbi vi\\\\u1ec7t_nam\", \"label\": 0}'\n",
      "produce message\n",
      "b'{\"author\": \"Ngoc bao Ngo\", \"datetime\": \"2021-10-21T06:00:41Z\", \"raw_comment\": \"tuy t\\\\u00f4i kh\\\\u00f4ng h\\\\u1ee9 th\\\\u00fa b\\\\u00f3ng \\\\u0111\\\\u00e1 nh\\\\u01b0 \\\\nC\\\\u00e1c b\\\\u1ea1n h\\\\u00e3y c\\\\u1ed1 l\\\\u00ean thua c\\\\u0169ng \\\\u0111\\\\u01b0\\\\u1ee3c kh\\\\u00f4ng sao c\\\\u1ea3 \\\\nmi\\\\u1ec5n l\\\\u00e0 kh\\\\u00f4ng b\\\\u1ecb tr\\\\u1ea5n th\\\\u01b0\\\\u01a1ng l\\\\u00e0 \\\\u0111\\\\u01b0\\\\u1ee3c \\\\nkh\\\\u00f4ng c\\\\u1ea7n ph\\\\u1ea3i c\\\\u1ed1 qu\\\\u00e1 l\\\\u00e0m j l\\\\u1ea1i \\\\u1ea3nh h\\\\u01b0\\\\u1edfng \\\\u0111\\\\u1ebfn s\\\\u1ee9c kho\\\\u1ebb th\\\\u00ec kh\\\\u1ed5 l\\\\u1eafm\", \"clean_comment\": \"tuy t\\\\u00f4i kh\\\\u00f4ng h\\\\u1ee9 th\\\\u00fa b\\\\u00f3ng_\\\\u0111\\\\u00e1 nh\\\\u01b0 c\\\\u00e1c b\\\\u1ea1n h\\\\u00e3y c\\\\u1ed1 l\\\\u00ean thua c\\\\u0169ng \\\\u0111\\\\u01b0\\\\u1ee3c kh\\\\u00f4ng sao c\\\\u1ea3 mi\\\\u1ec5n_l\\\\u00e0 kh\\\\u00f4ng b\\\\u1ecb tr\\\\u1ea5n th\\\\u01b0\\\\u01a1ng l\\\\u00e0 \\\\u0111\\\\u01b0\\\\u1ee3c kh\\\\u00f4ng c\\\\u1ea7n ph\\\\u1ea3i c\\\\u1ed1 qu\\\\u00e1 l\\\\u00e0m g\\\\u00ec l\\\\u1ea1i \\\\u1ea3nh_h\\\\u01b0\\\\u1edfng \\\\u0111\\\\u1ebfn s\\\\u1ee9c_kho\\\\u1ebb th\\\\u00ec kh\\\\u1ed5 l\\\\u1eafm\", \"label\": 0}'\n",
      "Stop flush!\n"
     ]
    }
   ],
   "source": [
    "producer = create_producer()\n",
    "response = youtube_service.commentThreads().list(\n",
    "      part='snippet',\n",
    "      maxResults=100,\n",
    "      textFormat='plainText',\n",
    "      order='time',\n",
    "      videoId=get_id(video_link)\n",
    "  ).execute()\n",
    "\n",
    "# response = youtube_service.liveChatMessages().list(\n",
    "#       part='snippet',\n",
    "#       maxResults=100,\n",
    "#       liveChatId=get_id(video_link)\n",
    "#   ).execute()\n",
    "\n",
    "results = response.get('items',[])\n",
    "\n",
    "# extract video comments\n",
    "try:\n",
    "    for item in results:\n",
    "        author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "        authorurl = item['snippet']['topLevelComment']['snippet']['authorChannelUrl']\n",
    "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        datetime = item['snippet']['topLevelComment']['snippet']['updatedAt']\n",
    "        \n",
    "        ## Hate speech detection\n",
    "        # load DNN model\n",
    "        model_path = os.path.abspath('../model/Text_CNN_model_PhoW2V.h5')\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        tknz_path = os.path.abspath('../model/tokenizer.pickle')\n",
    "        with open(tknz_path,\"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "            \n",
    "        # dnn\n",
    "        processed_comment = preprocessing(comment)\n",
    "        seq_comment = tokenizer.texts_to_sequences([processed_comment])\n",
    "        ds_comment = sequence.pad_sequences(seq_comment,maxlen=80)\n",
    "        pred = model.predict(ds_comment)\n",
    "        hsd_dt = pred.argmax(-1)\n",
    "        \n",
    "        record = {\"author\":author,\"datetime\":datetime,\"raw_comment\":comment,\n",
    "                  \"clean_comment\":processed_comment,\"label\":int(hsd_dt[0])}\n",
    "        record = json.dumps(record).encode(\"utf-8\")\n",
    "        print('produce message')\n",
    "        print(record)\n",
    "\n",
    "        producer.produce(topic=\"detected\",value=record)\n",
    "        producer.flush(30)\n",
    "        time.sleep(5)\n",
    "except KeyboardInterrupt:\n",
    "        print('Stop flush!')\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
