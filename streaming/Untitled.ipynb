{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf5e564-25b0-4d17-8415-fa90de3b4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "import regex as re\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO \n",
    "from pyvi import ViTokenizer, ViPosTagger,ViUtils\n",
    "import joblib\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "778db413-6a98-47f2-821c-5530f5c9bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotations(dataset):\n",
    "  pos = []\n",
    "  max_len = 8000\n",
    "  for i in range(dataset.shape[0]):\n",
    "    n = len(dataset.at[i,'cmt'])\n",
    "    l = [0] * max_len\n",
    "    for j in range(dataset.at[i,'start_index'], dataset.at[i,'end_index']):\n",
    "      l[j] = 1\n",
    "    pos.append(l)\n",
    "  return pos\n",
    "\n",
    "\n",
    "def abbreviation_predict(t):\n",
    "  model_path = os.path.abspath('../model/abb_model.sav')\n",
    "  loaded_model = joblib.load(model_path)\n",
    "\n",
    "  da_path = os.path.abspath('../dictionary/abbreviation_dictionary_vn.xlsx')\n",
    "  train_path = os.path.abspath('../dictionary/train_duplicate_abb_data.xlsx')\n",
    "  dev_path = os.path.abspath('../dictionary/dev_duplicate_abb_data.xlsx')\n",
    "  test_path = os.path.abspath('../dictionary/test_duplicate_abb_data.xlsx')\n",
    "  duplicate_abb = pd.read_excel(da_path,sheet_name='duplicate',header=None)\n",
    "  duplicate_abb = list(duplicate_abb[0])\n",
    "    \n",
    "  train_duplicate_abb_data = pd.read_excel(train_path)\n",
    "  dev_duplicate_abb_data = pd.read_excel(dev_path)\n",
    "  test_duplicate_abb_data = pd.read_excel(test_path)\n",
    "  duplicate_abb_data = pd.concat([train_duplicate_abb_data, dev_duplicate_abb_data, test_duplicate_abb_data], ignore_index=True)\n",
    "  duplicate_abb_data = duplicate_abb_data.drop_duplicates(keep='last').reset_index(drop=True)\n",
    "\n",
    "  X = duplicate_abb_data[['abb','start_index','end_index','cmt']]\n",
    "  y = duplicate_abb_data['origin']\n",
    "\n",
    "  from sklearn import preprocessing\n",
    "  le = preprocessing.LabelEncoder()\n",
    "  y = le.fit_transform(y)\n",
    "  enc = DictVectorizer()\n",
    "  Tfidf_vect = TfidfVectorizer(max_features=1200)\n",
    "\n",
    "  temp = annotations(X)\n",
    "  X_pos = sparse.csr_matrix(np.asarray(temp))\n",
    "  X_abb = enc.fit_transform(X[['abb']].to_dict('records'))\n",
    "  X_text= Tfidf_vect.fit_transform(X['cmt'])\n",
    "  X = hstack((X_abb,X_pos,X_text))\n",
    "\n",
    "  text = str(t)\n",
    "  max_len = 8000\n",
    "  if len(t)>max_len:\n",
    "    text =t[:max_len]\n",
    "    \n",
    "  cmt = ' ' + text + ' '\n",
    "  for abb in duplicate_abb:   \n",
    "    start_index = 0\n",
    "    count = 0\n",
    "    while start_index >-1: #start_index = -1 -> abb is not in cmt\n",
    "      start_index = cmt.find(' '+abb+' ')    #find will return FIRST index abb in cmt  \n",
    "      if start_index > -1:      \n",
    "        end_index = start_index + len(abb)\n",
    "        t = pd.DataFrame([[abb, start_index, end_index, text]], columns=['abb', 'start_index', 'end_index','cmt'],index=None)\n",
    "        temp = annotations(t)\n",
    "        X_pos = sparse.csr_matrix(np.asarray(temp))\n",
    "        \n",
    "        X_abb = enc.transform(t[['abb']].to_dict('records'))\n",
    "        # print(t['cmt'])\n",
    "        X_text= Tfidf_vect.transform([text])\n",
    "        \n",
    "        X = hstack((X_abb,X_pos,X_text))\n",
    "        predict = loaded_model.predict(X)\n",
    "        origin = le.inverse_transform(predict.argmax(axis=1))\n",
    "        origin = ''.join(origin)\n",
    "        text = text[:start_index+count*(len(origin)-len(abb))]+origin+text[end_index+count*(len(origin)-len(abb)):]\n",
    "        text = ''.join(text)\n",
    "        count = count + 1\n",
    "        for i in range(start_index+1,end_index+1):#replace abb to space ' '\n",
    "          cmt = cmt[:i] + ' '+ cmt[i+1:]        \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01bf621c-ef27-4a2d-afa9-640812e4a8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1:54 abe nha bắt cô panda dội nước đái :))))'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbreviation_predict('1:54 abe nha bắt cô panda dội nc đái :))))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9bc87d-473a-43ce-847d-b41103ed4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknz_path = os.path.abspath('../model/tokenizer.pickle')\n",
    "with open(tknz_path,\"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e436705-1d69-4469-bef8-45d4aa8d11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'Chúng ta đã đi qua quãng thời gian đẹp nhất của nhau. Xa nhau là điều 2 đứa không hề muốn. Thôi thì coi như mình có duyên gặp nhau mà không đủ phận ở bên nhau. Em nợ anh 1 câu yêu thương cho mai này, xin hẹn nhau một kiếp sống khác ta sum vầy... Bài hát đúng tâm trạng. Cảm ơn nhé. ☘️ ☘️ ☘️. Chúng ta đã đi qua quãng thời gian đẹp nhất của nhau. Xa nhau là điều 2 đứa không hề muốn. Thôi thì coi như mình có duyên gặp nhau mà không đủ phận ở bên nhau. Em nợ anh 1 câu yêu thương cho mai này, xin hẹn nhau một kiếp sống khác ta sum vầy... Bài hát đúng tâm trạng. Cảm ơn nhé. ☘️ ☘️ ☘️'\n",
    "test = preprocessing(test)\n",
    "test = tokenizer.texts_to_sequences([test])\n",
    "test = sequence.pad_sequences(test,maxlen=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7921a1b-330d-454c-8ac2-8a2aaf5a793b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cho mai này xin hẹn nhau một kiếp sống khác ta sum_vầy bài hát đúng tâm_trạng cảm_ơn nhé ️ ️ ️ chúng_ta đã đi quá quãng thời_gian đẹp nhất của nhau xa nhau là điều đứa không hề muốn thôi_thì coi như mình có duyên gặp nhau mà không đủ phận ở bên nhau em nợ anh câu yêu_thương cho mai này xin hẹn nhau một kiếp sống khác ta sum_vầy bài hát đúng tâm_trạng cảm_ơn nhé ️ ️ ️']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0cb318-68ac-4697-8ad2-e15a9d8aab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.abspath('../model/Text_CNN_model_PhoW2V.h5')\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc07b170-f6ce-4e3d-8886-5fdf08eb737c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1911226 , 0.63787633, 0.17100114]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ebf2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huy\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator Ridge from version 0.22.2.post1 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'huy là ta'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'Huy là ta'\n",
    "preprocessing(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725dbe77-d31c-4d38-8a91-9f14c0d28e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mọi người nghe rõ không'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing('mn nghe rõ không :)))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1564e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_path = os.path.abspath('../dictionary/abbreviation_dictionary_vn.xlsx')\n",
    "duplicate_abb = pd.read_excel(da_path,sheet_name='duplicate',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1b0881-6cd1-4b09-9379-bde29721c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30e8552-da9e-4101-9f49-3ed41a7164ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0837010b-ad29-431d-b569-2e210ccfc2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name of Python script: C:\\Users\\Huy\\anaconda3\\envs\\Homura2\\lib\\site-packages\\ipykernel_launcher.py\n",
      "\n",
      "Arguments passed: -f C:\\Users\\Huy\\AppData\\Roaming\\jupyter\\runtime\\kernel-e800c74a-48fd-4744-9655-32cf5ed63009.json "
     ]
    }
   ],
   "source": [
    "# Arguments passed\n",
    "print(\"\\nName of Python script:\", sys.argv[0])\n",
    " \n",
    "print(\"\\nArguments passed:\", end = \" \")\n",
    "for i in range(1, n):\n",
    "    print(sys.argv[i], end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aaded8f-80ef-434c-a4f1-67b9f43b48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a1a2b7-7a67-4194-8b3a-ec51c1fa10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = (x*x for x in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa26bb9b-05e7-4bf0-bb83-40c202b8f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in gen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc348fe-2c07-4eb6-8fb2-478f0afc1a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def gen():\n",
    "    for i in range(3):\n",
    "        yield i*i\n",
    "\n",
    "gene = gen()\n",
    "for i in gene:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8584e20b-4984-463f-81fa-5085ac793b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in gene:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
